{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import urllib\n",
    "import scipy.optimize\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem.porter import *\n",
    "from sklearn import linear_model\n",
    "import pickle\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseData(fname):\n",
    "    for l in open(fname):\n",
    "        yield eval(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading data...\")\n",
    "data = list(parseData(\"yelp_training_set_review.json\"))\n",
    "print(\"done\")\n",
    "\n",
    "train_set = data[:200000]\n",
    "test_set = data[200000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of unique bigrams =  3120774\n",
      "the number of unique unigrams =  205981\n"
     ]
    }
   ],
   "source": [
    "### Ignore capitalization and remove punctuation\n",
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "bigramCounter = defaultdict(int)\n",
    "unigramCounter = defaultdict(int)\n",
    "reviews = []\n",
    "\n",
    "for d in train_set:\n",
    "    r = ''.join([c for c in d['text'].lower() if not c in punctuation])\n",
    "    reviews.append(r)\n",
    "    allWords = r.split()\n",
    "    for w in r.split():\n",
    "        unigramCounter[w] += 1\n",
    "    for i in range(1, len(allWords)):\n",
    "        word1 = allWords[i - 1]\n",
    "        word2 = allWords[i]\n",
    "        bigramCounter[word1 + \" \" + word2] += 1\n",
    "\n",
    "print('the number of unique bigrams = ', len(bigramCounter))\n",
    "print('the number of unique unigrams = ', len(unigramCounter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 most frequently occurring unigrams = [(1333785, 'the'), (869970, 'and'), (723860, 'i'), (709764, 'a'), (602827, 'to'), (413064, 'of'), (403915, 'was'), (343766, 'it'), (343663, 'is'), (304557, 'for'), (294277, 'in'), (238356, 'that'), (224749, 'my'), (216843, 'with'), (212645, 'but'), (206895, 'this'), (203836, 'you'), (187123, 'they'), (184673, 'on'), (176520, 'have')]\n",
      "20 most frequently occurring bigrams = [(88972, 'of the'), (79341, 'it was'), (76992, 'in the'), (74682, 'and the'), (61262, 'this place'), (57373, 'on the'), (53955, 'i was'), (53588, 'and i'), (45569, 'to the'), (44849, 'the food'), (44473, 'for the'), (43837, 'i have'), (43485, 'for a'), (38949, 'is a'), (38663, 'i had'), (36925, 'if you'), (35793, 'to be'), (32973, 'with the'), (32930, 'with a'), (32725, 'at the')]\n"
     ]
    }
   ],
   "source": [
    "unigramCount = [(unigramCounter[w], w) for w in unigramCounter]\n",
    "unigramCount.sort()\n",
    "unigramCount.reverse()\n",
    "\n",
    "print('20 most frequently occurring unigrams =', unigramCount[:20])\n",
    "\n",
    "\n",
    "bigramCount = [(bigramCounter[w], w) for w in bigramCounter]\n",
    "bigramCount.sort()\n",
    "bigramCount.reverse()\n",
    "\n",
    "print('20 most frequently occurring bigrams =', bigramCount[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# most 1000 bigrams vector                       #\n",
    "##################################################\n",
    "bigramWords_sample = [x[1] for x in bigramCount[:1000]]\n",
    "bigramWordId_sample = dict(zip(bigramWords_sample, range(len(bigramWords_sample))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(datum):\n",
    "    feat = [0] * len(bigramWords_sample)\n",
    "    r = ''.join([c for c in datum['text'].lower() if not c in punctuation])\n",
    "    allWords = r.split()\n",
    "    bigramWordsData = []\n",
    "    for i in range(1, len(allWords)):\n",
    "        word1 = allWords[i - 1]\n",
    "        word2 = allWords[i]\n",
    "        bigramWordsData.append(word1 + \" \" + word2)\n",
    "    for w in bigramWordsData:\n",
    "        if w in bigramWords_sample:\n",
    "            feat[bigramWordId_sample[w]] += 1\n",
    "    feat.append(1)  # offset\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [feature(d) for d in train_set]\n",
    "y_train = [d['stars'] for d in train_set]\n",
    "X_test = [feature(d) for d in test_set]\n",
    "y_test = [d['stars'] for d in test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new predictor mean_squared_error using 1000 most common bigrams = 1.017259375190573\n"
     ]
    }
   ],
   "source": [
    "# With regularization\n",
    "clf = linear_model.Ridge(1.0, fit_intercept=False)\n",
    "clf.fit(X_train, y_train)\n",
    "theta = clf.coef_\n",
    "predictions = clf.predict(X_test)\n",
    "print('new predictor mean_squared_error using 1000 most common bigrams =', mean_squared_error(predictions, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigramCount = [(unigramCounter[w], w) for w in unigramCounter]\n",
    "bigramCount = [(bigramCounter[w], w) for w in bigramCounter]\n",
    "countBothWord = unigramCount + bigramCount\n",
    "countBothWord.sort()\n",
    "countBothWord.reverse()\n",
    "bothWords = [x[1] for x in countBothWord[:2000]]\n",
    "both_key_freq = dict(zip(bothWords, range(len(bothWords))))\n",
    "both_freq_key = dict(zip(range(len(bothWords)), bothWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(datum):\n",
    "    feat = [0] * len(bothWords)\n",
    "    r = ''.join([c for c in datum['text'].lower() if not c in punctuation])\n",
    "    words_temp = r.split()\n",
    "    bigramWordsData = []\n",
    "    for i in range(1, len(words_temp)):\n",
    "        word1 = words_temp[i - 1]\n",
    "        word2 = words_temp[i]\n",
    "        bigramWordsData.append(word1 + \" \" + word2)\n",
    "    for w in words_temp:\n",
    "        if w in bothWords:\n",
    "            feat[both_key_freq[w]] += 1\n",
    "    for w in bigramWordsData:\n",
    "        if w in bothWords:\n",
    "            feat[both_key_freq[w]] += 1\n",
    "    feat.append(1)  # offset\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [feature(d) for d in train_set]\n",
    "y_train = [d['stars'] for d in train_set]\n",
    "X_test = [feature(d) for d in test_set]\n",
    "y_test = [d['stars'] for d in test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new predictor mean_squared_error using both 2000 most common unigram and bigrams = 0.8210778176158229\n"
     ]
    }
   ],
   "source": [
    "# With regularization\n",
    "clf = linear_model.Ridge(1.0, fit_intercept=False)\n",
    "clf.fit(X_train, y_train)\n",
    "theta = clf.coef_\n",
    "predictions = clf.predict(X_test)\n",
    "print('new predictor mean_squared_error using both 2000 most common unigram and bigrams =', mean_squared_error(predictions, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "#  the idea of visulization of ratings           #\n",
    "##################################################\n",
    "\n",
    "weightCounter = []\n",
    "for i in range(len(theta) - 1):\n",
    "    weightCounter.append((theta[i], i))\n",
    "weightCounter.sort()\n",
    "weightCounter.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most positive 20 statement(unigrams and bigrams) in sense of stars rating:\n",
      "outstanding\n",
      "5 stars\n",
      "love this\n",
      "incredible\n",
      "excellent\n",
      "awesome\n",
      "die\n",
      "amazing\n",
      "cant wait\n",
      "fantastic\n",
      "helpful\n",
      "never had\n",
      "better than\n",
      "i never\n",
      "great food\n",
      "not only\n",
      "wonderful\n",
      "highly recommend\n",
      "will definitely\n",
      "best\n",
      "most negative 20 statement(unigrams and bigrams) in sense of stars rating:\n",
      "worst\n",
      "horrible\n",
      "rude\n",
      "terrible\n",
      "mediocre\n",
      "overpriced\n",
      "poor\n",
      "bland\n",
      "dirty\n",
      "will not\n",
      "sorry\n",
      "wont be\n",
      "money\n",
      "unfortunately\n",
      "slow\n",
      "would not\n",
      "dry\n",
      "average\n",
      "greasy\n",
      "guess\n"
     ]
    }
   ],
   "source": [
    "print('most positive 20 statement(unigrams and bigrams) in sense of stars rating:')\n",
    "for i in range(20):\n",
    "    print(both_freq_key[weightCounter[i][1]])\n",
    "\n",
    "weightCounter.reverse()\n",
    "\n",
    "print('most negative 20 statement(unigrams and bigrams) in sense of stars rating:')\n",
    "for i in range(20):\n",
    "    print(both_freq_key[weightCounter[i][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "#  with 2000-dimensional tf-idf representations  #\n",
    "##################################################\n",
    "\n",
    "temp = defaultdict(list)\n",
    "freq_temp = defaultdict(int)\n",
    "\n",
    "words = []\n",
    "for r in reviews:\n",
    "    words += r.split()\n",
    "uniqueWords = set(words)\n",
    "for i in range(len(reviews)):\n",
    "    r = reviews[i]\n",
    "    for w in r.split():\n",
    "        if len(temp[w]) == 0 or i != temp[w][-1]:\n",
    "            temp[w].append(i)\n",
    "\n",
    "for w in uniqueWords:\n",
    "    freq_temp[w] = len(temp[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calTfIdf(word, r):\n",
    "    tf = 0\n",
    "    r_temp = r.split()\n",
    "    for w in r_temp:\n",
    "        if word == w:\n",
    "            tf += 1\n",
    "    N = len(reviews)\n",
    "\n",
    "    idf = np.log10(N * 1.0 / freq_temp[word])\n",
    "    tfidf = tf * idf\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = [(unigramCounter[w], w) for w in unigramCounter]\n",
    "counts = countBothWord\n",
    "counts.sort()\n",
    "counts.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [x[1] for x in counts[:2000]]\n",
    "wordId = dict(zip(words, range(len(words))))\n",
    "wordSet = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(datum):\n",
    "    feat = [0] * len(words)\n",
    "    r = ''.join([c for c in datum['text'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        if w in words:\n",
    "\n",
    "\n",
    "            feat[wordId[w]] = calTfIdf(w, r)\n",
    "\n",
    "    feat.append(1)  # offset\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [feature(d) for d in train_set]\n",
    "y_train = [d['stars'] for d in train_set]\n",
    "X_test = [feature(d) for d in test_set]\n",
    "y_test = [d['stars'] for d in test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With regularization\n",
    "clf = linear_model.Ridge(1.0, fit_intercept=False)\n",
    "clf.fit(X_train, y_train)\n",
    "theta = clf.coef_\n",
    "predictions = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the new model with 1000-dimensional unigrams and 1000-dimensional bigrams tf-idf representations MSE = 0.8210518201753847\n"
     ]
    }
   ],
   "source": [
    "print('the new model with 1000-dimensional unigrams and 1000-dimensional bigrams tf-idf representations MSE =', mean_squared_error(predictions, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
